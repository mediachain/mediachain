# RFC 4: New Directions in Mediachain: Protocols and Architecture

Status: WIP WIP WIP WIP

Authors: vyzo, parkan

## Lessons from Phase I: It's not a Blockchain

The blockchain is an attractive solution: the public ledger promotes
open data and discovery, while also simplifying streaming for online
processing.

But it comes with significant costs:

* Complexities of maintaining consensus in a peer to peer network.
* PoS/PoW economics that don't quite work in our domain.
* Inefficient querying, requires the entire blockchain for read operations.
* Difficult to scale to large datasets with bulk and firehose ingestion,
  the blockchain becomes the bottleneck.
* Ultimately trying to solve a problem we don't have: double spending.

In reality, we don't need a singular linearly ordered view of the world.
And we want to allow the system to grow to very large datasets (billions
of objects) supporting bulk and firehose ingestion models.

Fundamentally, what we need is a map of media identifiers to sets
of statements that allows us to track and relate media objects.
This has the hallmarks of a CRDT data structure, which allows a system to
achieve eventually consistent state without the need for consensus. 

So what we want to build is a distributed database that supports
_upserts_ connecting statements to one or more domain specific
identifiers. An upsert is an idempotent insert/update operation that
does not require knowledge of the current state of the database. This
allows operations to execute concurrently without requiring locks
or ordering.

It is intended to provide a low level plumbing data structure, as dumb
as possible. Rich relations between objects can be expressed with
merkle DAGs in object content, allowing the application layer to
evolve according to user needs.

### Domain-Specific Identifiers Instead of Opaque Pointers

In the Phase I design, object identifiers were mediachain-issued
opaque pointers generated by content hashes of the initial insert.
Moving forward, we choose to utilize well-known/domain-specific
identifiers (WKI; e.g. moma:190935) with the following motivations:

* It reduces burden of authoritative ID issuance (e.g. DOI) as well as
  authoritatively asserting canonical-instance relationships.
* Most works we are dealing with at this stage will already have an ID established
  by previous use.
* Mapping from a set of WKIs to a set of statements is an elegant way of
  granularly and nondestructively expressing sameness/merges, with potentially
  different "sameness" perspectives represented through namespace relationships.
  For instance, a service like Clarifai can publish a namespace containing only
  merges based on visual similarity
* WKI-based provider records can be reasonably implemented as distributed,
  freeing organizationss from relying on indexers for simple mappings

Works that are mediachain-first and require an identifier can either
generate one or receive it from a ticket server. This ID will be
treated the same way as any other.

### Data Structure Operations

The Phase I data structure interface provided 3 basic operations:
insert, update, and lookup. The insert method ingested an object into
the system and returned its canonical identifier. For each identifier,
the system maintained a chain pointer linking the object to a chain of
updates. The update method updated the chain pointer by consing a new
update object on top of the current chain, while lookup retrieved the
head of the chain for a canonical.

In practice, inserts are idempotent and the chained update model only
serves to constrain the system by imposing order of operations.
By dropping ordering and utilizing domain specific identifiers, we can
eliminate the distinction between insert and update.

The write interface is thus an upsert method, which combines insert
and update, and adds a statement to the set of objects associated with
an identifier. If the system has no prior knowledge of the identifier,
then it creates a new entry in the identifier mapping with a singleton
set containing the object.

The lookup method can be retained as a basic read operation, with a
change at its return value. Since there are no chained updates any
more, the method returns the known set of updates relating to an
identifier. 

### Namespace Partitioning

In order to reflect the natural organization of data according to
institution, industry, topic, etc, we introduce the concept of
namespaces. In terms of scaling, the partitioning must be semantic so
as to allow peers to commit resources only in topics they are
interested in. In terms of administrative control, the partitioning
must be hierarchical so that we can easily delegate management and
moderation to organizational stake holders.

To satisfy these requirements, we've chosen to adopt a hierarchical
structure similar to unix paths. Leaves in the hierarchy map to
specific datasets and provide the primary entry points for writes, and
namespaces higher in the hierarchy aggregate namespaces below and
provide a progressively expanded read view of the data structure. Much
like symlinks with unix paths, alternate views of the hierarchy are
possible, namespaces may have multiple parents, and so on. This
creates a semantically rich and expressive framework for querying and
composition.

### Moderation and Publishing Model

Namespaces allow us fine grained control on publishing and
participation level. At one end of the spectrum, we want to
encourage public namespaces which allow lightweight permissionless
participation. At the other end, we want to support curated namespaces
to combat spam and allow authoritative sources and commercial media
providers to maintain control of their own data.

In order to implement this model, each participating peer has an
associated identity corresponding to a public key. The identities
are intended to be long-lived and eventually have an attached reputation
score.

All statements published in the system are signed with the key of the
source of the statement. In order to publish in a moderated namespace,
the originating peer must present a certificate which allows it to
publish there. Alternatively, it can convince a peer with the right
certificate to publish on its behalf. Certificates can be obtained by
moderators and owners of the namespace. 

This moderation scheme, with delegated administrative control,
attaches a cost to peer identities because certificates can be
revoked. This encourages cooperative behavior by individual peers,
thus avoiding the social cost of cheap identities [1]. At the same
time, individual peers with publishing rights are free to implement
their own authentication for their clients.

### Queries and Aggregation

Namespaces provide a convenient way to request information about the
same object from several different strata. For example, a query for an
image from the NYPL collection and the corresponding OldNYC geocoding
data can look like

```
{nypl:510d47e2-ef28-a3d9-e040-e00a18064a99@nypl,oldnyc}
```

which would mean "return results for WKI nypl:510d47e2-ef28-a3d9-e040-e00a18064a99 from
[nypl](http://digitalcollections.nypl.org/items/510d47e2-ef28-a3d9-e040-e00a18064a99)
and [oldnyc](https://www.oldnyc.org/#1557929) namespaces". NYPL can
also choose to transclude the statment from the `oldnyc` namespace
into `nypl` itself, "blessing" it.

Other possibilities include filtered aggregation (a `lithographs`
namespace that aggregates some objects from `getty`, some from `nypl`,
etc), lazy aggregation (logical aggregates that do not publish a
stream until requested), etc. Exact semantics TBD.


## A Heterogeneous Network of Cooperative Peers

The distributed database is maintained by a heterogeneous peer-to-peer
network. Peers are operated by different organizations and
individuals, and contribute different resources to the network.

Each peer maintains some parts of the dataset and has a limited view
of the mediachain. Peers synchronize their state with peer-to-peer
interactions, and converge their view of the data structure by
exchanging published statements.

Statements can be propagated in the network via push, either with
direct messages or through streaming. For stream propagation, stable
peers form pubsub overlays for their namespaces and emit published
statements to their subscribers. Equally well, peers can poll their
peers for recent updates, allowing them to operate in a disconnected
fashion.

The metadata associated with statements is stored in IPFS using the
IPLD format, with the seeding initially supplied by sources. As
statements propagate in the network, peers may opportunistically
reseed metadata in order to aid availability and distribution. More
specialized peers can provide persistent seeding and achiving services
for some namespaces, potentially incentivized through something like
Filecoin.

### Peer Roles

Peers can take on a variety of roles like intermittent sources,
firehose publishers, aggregators, indexers, archivers, etc. See the
[Peer Roles](mediachain-rfc-4-roles.md) supplement for specific examples
and case studies.

From an operational perspective, the basic possible role actions are
read/receive, write/publish and aggregate/republish. A peer acts as a
reader when it receives new statements from the network and adds it to
its local store. A peer acts as a writer when it publishes new
statements to the network, which eventually get distributed to
interested readers. Finally, a peer acts as an aggregator when it
reads from multiple namespaces and republishes into an aggregate
namespace.

### The Local Store

Each peer has a local store which serves as a spool for all statements
it has created or received by other peers. Using the local store, the
peer can compute the index that maps identifiers to metadata
statements and answer basic queries.

Peers can operate disconnected and simply manipulate their local view
of the mediachain by adding new statements to the store. Their updates
are buffered in the spool until they connect to the network and
publish them to other peers.

### Statement Publication

In order to propagate statements in the network and synchronize its
local store, a peer needs to discover other peers with publishing
permissions in relevant namespaces. Directory servers facilitate this
interaction: every peer who stays online registers with a directory
server for its namespaces. Thus, a peer just coming online can obtain
a list of peers who are interested in its buffered statements and
can be polled to provide new statements.

A publishing peer who intends to stay online can further proceed to
register with the directory. This makes it discoverable by other peers
who want to publish or read statements in its namespaces.

At this level, the system can already simply work asynchronously, by
having readers periodcally poll publishers for new statements and
writers push new statements to other publishers. It is sufficient to
have a stable population of online publishers, who ensure the eventual
publication of new statements to all readers.

In order to streamline online publication and indexing, stable
publishers can organize into pubsub overlays. The overlay for a
namespace can be constructed by connecting to a subset of other
publishers in a way that maintains connectivity with some redundancy
factor. Readers and aggregators can then connect as downstream clients
from publishers.

Deeper overlay topologies can be constructed by allowing stable
readers to serve as intermediate nodes who relay statements to other
readers (or relays). This is a safe operation, as each statement
carries the signature of a publisher and peers can verify that
signature and associated certificates independently.

### Metadata Ingestion

A fundamental use case for the system is the ability to ingest
pre-existing datasets from authoritative sources in bulk.

The ingestion process first requires a translation step, where
metadata records from the existing dataset are translated to
structured metadata objects for the mediachain. The next step is to
publish the objects in IPFS and obtain their pointers, so that they
can related to media identifiers with statements. Finally, for each
object in the original dataset a statement msut be generated, which
binds the pre-existing object identifier to the metadata object with a
cryptographic signature.

This process follows a straightforward path from existing record to
mediachain statement and may very well work with small datasets.
However, this process is extremely inefficient for large datasets
with millions of artefacts. Firstly, the metadata need to published to
IPFS, a process which may be lengthy and resource hungry. Secondly, a
cryptographic signature needs to be generated for each statement, a
process which is also computationally expensive. And then there is the
traffic required for propagating statements in the network, which can
overload caches and online processors.

In order to address the cost of signatures, the protocol allows for
binding multiple statements together with a single signature. And in
order to address the load issues, the protocol allows for publishing
archives to the network. These archives are tarballs that contain
statements and their associated metadata as separate files named by
their IPLD hashes. A bulk ingesting source can thus avoid the IPFS
publication step and bundle the statements directly with their
metadata objects.  Furthermore, the archive can be distributed
directly to interested readers by using a protocol optimized for bulk
transfer, such as IPFS Bitswap or Bittorrent.

Similar dynamics arise for the use case of firehose sources, which
generate a constant load of new statements, eg by following social
media feeds.  Firehoses don't have to publish every statement in real
time. Instead, they can buffer statements and periodically publish
them together with their metadata as archives. This allows the system
to push high volumes of data without overloading the network.

### Metadata Persistence

The flow of metadata in the system follows the path from sources to
readers through IPFS. When a reader receives a statement and adds it
to its local store, it also fetches the associated metadata. Client
readers are not expected to reseed the object in IPFS, especially if
they operate in a disconnected fashion.

In order to avoid overloading the source with IPFS requests and
provide availability even when the source is offline, the metadata is
reseeded by caches who follow the relevant namespaces. Longer term
metadata persistence in the form of archive files is provided by
archivers. These basic services will be supported by organizations
using the mediachain per normal course of operation.

The issue of persistent seeding in IPFS is a little more complicated.
Firstly, a seeder must consume memory, storage, and egress bandwidth
in order to serve the objects. Secondly, IPFS publishing creates
system traffic in the form of DHT provider announcements. Thus, a form
of incentives needs to be devised to encourage and justify persistent
IPFS seeding for high value namespace. This can be achieved with an
application-specific coin like filecoin or through a well established
blockchain.

### Indexes and Queries

In order to query the database, a peer can construct a local index by
processing statements in the local store. This index provides the
local view of the mediachain, and allows retrieval of statements and
metadata objects based on multiple criteria.

A basic indexing scheme, which performs only statement envelope processing
and doesn't parse the metadata, allows retrieval with the following
criteria:

- Query by WKI, publisher or peer id, statement id, object id
- Query by namespace(s)
- Query by timestamp

This baseline index, which can be easily implemented using an embedded
db like sqlite, is the simplest way to implement polling for state
synchronization. It also provides a low level interface for advanced
users, who are free to develop and share more advanced indexing schemes
that parse object content.

Queries do not have to be limited to one's local view however. For
instance, a user may want to query joining on namespaces it is not
currently following.  This could be because the user is exploring the
namespace or because the particular namespace has a larger index that
aggregates multiple namespaces. This can be accomplished by directly
querying other peers or by utilizing more specialized indexers.

Specialized indexer peers will be deployed per normal course of
operation. These indexers will be discoverable through directory
servers and do not have to be limited to basic envelope
processing. They can parse the metadata and offer a more generalized
query language that searches object content and can take advantage of
semantic relations expressed in the application layer.

### Database Semantics

At this point, it is also worth considering the consistency,
availability, and partition tolerance properties of our database.  The
ability of the system to handle disconnected operation with irregular
synchronization gives it obvious partition tolerance, and the local
store makes the system always available. What we can't have though, is
global consistency.

Nonetheless, the core data structure is commutative and monotonic
which gives it excellent convergence properties. This allows peers to
efficiently synchronize their local state through peer-to-peer
interactions. The overall consistency model of the database is
eventual consistency, even if radically so: in the absence of new
updates, all live peers will eventually converge to the same state with
a finite number of messages.

### Governance and Public Namespaces

An important aspect of the system is the governance of the namespace.
Moderated namespaces can be effectively governed initially be decree,
and delegated administrative control is a reasonable model that
can work with multiple independent stakeholders at large.

The governance of public namespace is a different problem. The intention
is to have an open system that encourages lightweight participation
and can grow a community of independent contributors. As the system
grows in popularity however, spam will inevitably appear and it can
quickly escalate to large volumes. This has the potential to damage the
community and diminish the utility of the system for its users.

There is a number of approaches to the problem of spam:

* Adopt a reactive stance and develop spam filters as the spam appears.
* Create a reputation-based system where users are vetted by other users.
* Establish an economy around publishing by utilizing a system like Filecoin
  to provide fuel for publishers.

Ultimately, it is too early to decide on the most effective approach.
For the Phase II development plan we can start with a completely
unmoderated public space, and create personal and community moderated
namespaces for active contributors. In the meantime we can study the
nature of metadata spam to see whether we can develop effective
filters, and develop the governance of public space with the
community's input.


---
WIP WIP WIP WIP

## The Mediachain Protocol

### Identity and Namespaces

#### Identities

#### Namespaces

#### Permission Model

#### Certificates


### Statement Publication

#### Statements

#### Archives

#### Publication Protocol

#### State Synchronization


### Queries


### Peer Discovery


## Examples

TBD

## References

1. [The Social Cost of Cheap Pseudonyms](http://www.haas.berkeley.edu/Courses/Spring2000/BA269D/FriedmanResnick99.pdf)
